#### 目录介绍

- 01.HashMap内部结构
- 02.HashMap构造函数
- 03.成员变量
- 04.put(K key, V value)
- 05.get(Object key)
- 06.remove(Object key)
- 07.resize()扩容
- 08.Hash函数实现
- 09.容量和装载因子
- 10.HashTable和HashMap
- 11.hashCode和equal
- 12.Key为何需要不可变
- 13.HashMap为啥要扩容
- 14.HashMap的table下标

### 01.HashMap内部结构

- HashMap内部的结构是怎样的
  - HashMap 内部的结构，它可以看作是数组（Node[] table）和链表结合组成的复合结构，数组被分为一个个桶（bucket），通过哈希值决定了键值对在这个数组的寻址；哈希值相同的键值对，则以链表形式存储，你可以参考下面的示意图。
  - 这里需要注意的是，如果链表大小超过阈值（TREEIFY_THRESHOLD, 8，图中的链表就会被改造为树形结构。[博客](https://github.com/yangchong211/YCBlogs)
- 结构图如下所示
  - ![image](https://upload-images.jianshu.io/upload_images/4432347-f43d61f063f453c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 02.HashMap构造函数

- HashMap构造函数如下所示
  
  ```java
  public HashMap(int initialCapacity, float loadFactor) {
      if (initialCapacity < 0)
          throw new IllegalArgumentException("Illegal initial capacity: " +
                                             initialCapacity);
      if (initialCapacity > MAXIMUM_CAPACITY)
          initialCapacity = MAXIMUM_CAPACITY;
      if (loadFactor <= 0 || Float.isNaN(loadFactor))
          throw new IllegalArgumentException("Illegal load factor: " +
                                             loadFactor);
      this.loadFactor = loadFactor;
      this.threshold = tableSizeFor(initialCapacity);
  }
  ```

```java
public HashMap(int initialCapacity) {
    this(initialCapacity, DEFAULT_LOAD_FACTOR);
}

/**
 * Constructs an empty <tt>HashMap</tt> with the default initial capacity
 * (16) and the default load factor (0.75).
 */
public HashMap() {
    this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
}
```

### 03.成员变量

- 成员变量如下所示

```java
  //哈希桶数组，在第一次使用时才初始化
  //容量值应是2的整数倍
  transient Node<K, V>[] table;

  /**
   * Holds cached entrySet(). Note that AbstractMap fields are used
   * for keySet() and values().
   */
  transient Set<Map.Entry<K, V>> entrySet;

  //Map的大小
  transient int size;

  //每当Map的结构发生变化时，此参数就会递增
  //当在对Map进行迭代操作时，迭代器会检查此参数值
  //如果检查到此参数的值发生变化，就说明在迭代的过程中Map的结构发生了变化，因此会直接抛出异常
  transient int modCount;

  //数组的扩容临界点，当数组的数据量达到这个值时就会进行扩容操作
  //计算方法：当前容量 x 装载因子
  int threshold;

  //使用的装载因子值
  final float loadFactor;
```

### 04.put(K key, V value)

- put函数大致的思路为：[博客](https://github.com/yangchong211/YCBlogs)
  
  - 1.对key的hashCode\(\)做hash，然后再计算index;
  - 2.如果没碰撞直接放到bucket里；
  - 3.如果碰撞了，以链表的形式存在buckets后；
  - 4.如果碰撞导致链表过长\(大于等于`TREEIFY_THRESHOLD`\)，就把链表转换成红黑树；
  - 5.如果节点已经存在就替换old value\(保证key的唯一性\)
  - 6.如果bucket满了\(超过`load factor*current capacity`\)，就要resize。

- put源码如下所示
  
  ```java
  public V put(K key, V value) {
      // 对key的hashCode()做hash
      return putVal(hash(key), key, value, false, true);
  }
  final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                 boolean evict) {
      Node<K,V>[] tab; Node<K,V> p; int n, i;
      // tab为空则创建
      if ((tab = table) == null || (n = tab.length) == 0)
          n = (tab = resize()).length;
      // 计算index，并对null做处理
      if ((p = tab[i = (n - 1) & hash]) == null)
          tab[i] = newNode(hash, key, value, null);
      else {
          Node<K,V> e; K k;
          // 节点存在
          if (p.hash == hash &&
              ((k = p.key) == key || (key != null && key.equals(k))))
              e = p;
          // 该链为树
          else if (p instanceof TreeNode)
              e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
          // 该链为链表
          else {
              for (int binCount = 0; ; ++binCount) {
                  if ((e = p.next) == null) {
                      p.next = newNode(hash, key, value, null);
                      if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                          treeifyBin(tab, hash);
                      break;
                  }
                  if (e.hash == hash &&
                      ((k = e.key) == key || (key != null && key.equals(k))))
                      break;
                  p = e;
              }
          }
          // 写入
          if (e != null) { // existing mapping for key
              V oldValue = e.value;
              if (!onlyIfAbsent || oldValue == null)
                  e.value = value;
              afterNodeAccess(e);
              return oldValue;
          }
      }
      ++modCount;
      // 超过load factor*current capacity，resize
      if (++size > threshold)
          resize();
      afterNodeInsertion(evict);
      return null;
  }
  ```

- put源码解析大概如下所示
  
  - 如果表格是 null，resize 方法会负责初始化它，这从 tab = resize() 可以看出。
  - resize 方法兼顾两个职责，创建初始存储表格，或者在容量不满足需求的时候，进行扩容（resize）。
  - 在放置新的键值对的过程中，如果发生if (++size > threshold)条件，就会发生扩容。
  - 具体键值对在哈希表中的位置（数组 index）取决于下面的位运算：i = (n - 1) & hash
  - 仔细观察哈希值的源头，我们会发现，它并不是 key 本身的hashCode，而是来自于 HashMap 内部的另外一个 hash 方法。注意，为什么这里需要将高位数据移位到低位进行异或运算呢？这是因为有些数据计算出的哈希值差异主要在高位，而 HashMap 里的哈希寻址是忽略容量以上的高位的，那么这种处理就可以有效避免类似情况下的哈希碰撞。

- 当有键值对插入时，HashMap会发生什么 ?
  
  - 首先，键的哈希值被计算出来，然后这个值会赋给 Entry 类中对应的 hashCode 变量。
  - 然后，使用这个哈希值找到它将要被存入的数组中“桶”的索引。
  - 如果该位置的“桶”中已经有一个元素，那么新的元素会被插入到“桶”的头部，next 指向上一个元素——本质上使“桶”形成链表。[博客](https://github.com/yangchong211/YCBlogs)

### 05.get(Object key)

- 在理解了put之后，get就很简单了。大致思路如下：

- 具体代码的实现如下：
  
  ```java
  public V get(Object key) {
      Node<K,V> e;
      return (e = getNode(hash(key), key)) == null ? null : e.value;
  }
  final Node<K,V> getNode(int hash, Object key) {
      Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
      if ((tab = table) != null && (n = tab.length) > 0 &&
          (first = tab[(n - 1) & hash]) != null) {
          // 直接命中
          if (first.hash == hash && // always check first node
              ((k = first.key) == key || (key != null && key.equals(k))))
              return first;
          // 未命中
          if ((e = first.next) != null) {
              // 在树中get
              if (first instanceof TreeNode)
                  return ((TreeNode<K,V>)first).getTreeNode(hash, key);
              // 在链表中get
              do {
                  if (e.hash == hash &&
                      ((k = e.key) == key || (key != null && key.equals(k))))
                      return e;
              } while ((e = e.next) != null);
          }
      }
      return null;
  }
  ```

- 大概的意思是
  
  - bucket里的第一个节点，直接命中；
  - 如果有冲突，则通过key.equals\(k\)去查找对应的entry
    - 若为树，则在树中通过key.equals\(k\)查找，O\(logn\)；
    - 若为链表，则在链表中通过key.equals\(k\)查找，O\(n\)。

- 对于查找一个key时，HashMap会发生什么 ?
  
  - 键的哈希值先被计算出来
  - 在 mHashes[] 数组中二分查找此哈希值。这表明查找的时间复杂度增加到了 O(logN)。
  - 一旦得到了哈希值所对应的索引 index，键值对中的键就存储在 mArray[2index] ，值存储在 mArray[2index+1]。[博客](https://github.com/yangchong211/YCBlogs)
  - 这里的时间复杂度从 O(1) 上升到 O(logN)，但是内存效率提升了。当我们在 100 左右的数据量范围内尝试时，没有耗时的问题，察觉不到时间上的差异，但我们应用的内存效率获得了提高。

### 06.remove(Object key)

- 从 Map 中移除键值对的操作，在底层数据结构的体现就是移除对某个结点对象的引用，可能是从数组中、也可能是链表或者红黑树。
  
  ```java
  public V remove(Object key) {
      Node<K, V> e;
      return (e = removeNode(hash(key), key, null, false, true)) == null ?
              null : e.value;
  }
  
  /**
   * Implements Map.remove and related methods
   *
   * @param hash       key 的哈希值
   * @param key        the key
   * @param value      key对应的值，只有当 matchValue 为 true 时才需要使用到，否则忽略该值
   * @param matchValue 如果为 true ，则只有当 Map 中存在某个键 equals key 且 value 相等时才会移除该元素，否则只要 key 的 hash 值相等就直接移除该元素
   * @param movable    if false do not move other nodes while removing
   * @return the node, or null if none
   */
  final Node<K, V> removeNode(int hash, Object key, Object value,
                              boolean matchValue, boolean movable) {
      Node<K, V>[] tab;
      Node<K, V> p;
      int n, index;
      //只有当 table 不为空且 hash 对应的索引位置存在值时才有可移除的对象
      if ((tab = table) != null && (n = tab.length) > 0 && (p = tab[index = (n - 1) & hash]) != null) {
          Node<K, V> node = null, e;
          K k;
          V v;
          //如果与头结点的 key 相等
          if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))))
              node = p;
          else if ((e = p.next) != null) { //存在哈希冲突
              //用红黑树来处理哈希冲突
              if (p instanceof TreeNode)
                  //查找对应结点
                  node = ((TreeNode<K, V>) p).getTreeNode(hash, key);
              else { //用链表来处理哈希冲突
                  do {
                      if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) {
                          node = e;
                          break;
                      }
                      p = e;
                  } while ((e = e.next) != null);
              }
          }
          //node != null 说明存在相应结点
          //如果 matchValue 为 false ，则通过之前的判断可知查找到的结点的 key 与 参数 key 的哈希值一定相等，此处就可以直接移除结点 node
          //如果 matchValue 为 true ，则当 value 相等时才需要移除该结点
          if (node != null && (!matchValue || (v = node.value) == value || (value != null && value.equals(v)))) {
              if (node instanceof TreeNode) //对应红黑树
                  ((TreeNode<K, V>) node).removeTreeNode(this, tab, movable);
              else if (node == p) //对应 key 与头结点相等的情况，此时直接将指针移向下一位即可
                  tab[index] = node.next;
              else //对应的是链表的情况
                  p.next = node.next;
              ++modCount;
              --size;
              //用于 LinkedHashMap ，在 HashMap 中是空实现
              afterNodeRemoval(node);
              return node;
          }
      }
      return null;
  }
  ```

### 07.resize()扩容

- 初始化或加倍表大小。
  
  - 如果为NULL，则按照在字段阈值中保持的初始容量目标分配。否则，由于我们使用的是二次幂展开，每个bin中的元素要么保持在相同的索引中，要么在新表中以两个偏移的幂移动。

- 依据 resize 源码，不考虑极端情况（容量理论最大极限由MAXIMUM_CAPACITY 指定，数值为1<<30，也就是 2 的 30 次方），可以归纳为：[博客](https://github.com/yangchong211/YCBlogs)
  
  - 门限值等于（负载因子）x（容量），如果构建 HashMap的时候没有指定它们，那么就是依据相应的默认常量值。
  - 门限通常是以倍数进行调整 （newThr = oldThr << 1），我前面提到，根据 putVal 的逻辑，当元素个数超过门限大小时，则调整 Map 大小。
  - 扩容后，需要将老的数组中的元素重新放置到新的数组，这是扩容的一个主要开销来源。

- resize()源码如下所示
  
  ```java
  final Node<K,V>[] resize() {
      Node<K,V>[] oldTab = table;
      int oldCap = (oldTab == null) ? 0 : oldTab.length;
      int oldThr = threshold;
      int newCap, newThr = 0;
      if (oldCap > 0) {
          if (oldCap >= MAXIMUM_CAPACITY) {
              threshold = Integer.MAX_VALUE;
              return oldTab;
          }
          else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                   oldCap >= DEFAULT_INITIAL_CAPACITY)
              newThr = oldThr << 1; // double threshold
      }
      else if (oldThr > 0) // initial capacity was placed in threshold
          newCap = oldThr;
      else {               // zero initial threshold signifies using defaults
          newCap = DEFAULT_INITIAL_CAPACITY;
          newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
      }
      if (newThr == 0) {
          float ft = (float)newCap * loadFactor;
          newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                    (int)ft : Integer.MAX_VALUE);
      }
      threshold = newThr;
      @SuppressWarnings({"rawtypes","unchecked"})
          Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
      table = newTab;
      if (oldTab != null) {
          for (int j = 0; j < oldCap; ++j) {
              Node<K,V> e;
              if ((e = oldTab[j]) != null) {
                  oldTab[j] = null;
                  if (e.next == null)
                      newTab[e.hash & (newCap - 1)] = e;
                  else if (e instanceof TreeNode)
                      ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                  else { // preserve order
                      Node<K,V> loHead = null, loTail = null;
                      Node<K,V> hiHead = null, hiTail = null;
                      Node<K,V> next;
                      do {
                          next = e.next;
                          if ((e.hash & oldCap) == 0) {
                              if (loTail == null)
                                  loHead = e;
                              else
                                  loTail.next = e;
                              loTail = e;
                          }
                          else {
                              if (hiTail == null)
                                  hiHead = e;
                              else
                                  hiTail.next = e;
                              hiTail = e;
                          }
                      } while ((e = next) != null);
                      if (loTail != null) {
                          loTail.next = null;
                          newTab[j] = loHead;
                      }
                      if (hiTail != null) {
                          hiTail.next = null;
                          newTab[j + oldCap] = hiHead;
                      }
                  }
              }
          }
      }
      return newTab;
  }
  ```

- 当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。
  
  - 在resize的过程，简单的说就是把bucket扩充为2倍，之后重新计算index，把节点再放到新的bucket中。resize的注释是这样描述的：
  - > Initializes or doubles table size. If null, allocates in accord with initial capacity target held in field threshold. Otherwise, because we are using power-of-two expansion, the elements from each bin must either**stay at same index**, or**move with a power of two offset**in the new table.
  - 大致意思就是说，当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展\(指长度扩为原来2倍\)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。

- 例如我们从16扩展为32时，具体的变化如下所示：  
  
  - ![](https://cloud.githubusercontent.com/assets/1736354/6958256/ceb6e6ac-d93b-11e4-98e7-c5a5a07da8c4.png "rehash")
  - 因此元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit\(红色\)，因此新的index就会发生这样的变化：  
  - ![](https://cloud.githubusercontent.com/assets/1736354/6958301/519be432-d93c-11e4-85bb-dff0a03af9d3.png "resize")
  - 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图：[博客](https://github.com/yangchong211/YCBlogs)
  - ![](https://cloud.githubusercontent.com/assets/1736354/6958677/d7acbad8-d941-11e4-9493-2c5e69d084c0.png "resize16-32")
  - 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。

### 08.Hash函数实现

- 首先看下存储put代码
  
  ```java
  public V put(K key, V value) {
      return putVal(hash(key), key, value, false, true);
  }
  ```

```java
static final int hash(Object key) {
    int h;
    //高位运算
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

```
- 可以看到这个函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或。其中代码注释是这样写的：[博客](https://github.com/yangchong211/YCBlogs)

  ```java
  /**
   * Computes key.hashCode() and spreads (XORs) higher bits of hash
   * to lower.  Because the table uses power-of-two masking, sets of
   * hashes that vary only in bits above the current mask will
   * always collide. (Among known examples are sets of Float keys
   * holding consecutive whole numbers in small tables.)  So we
   * apply a transform that spreads the impact of higher bits
   * downward. There is a tradeoff between speed, utility, and
   * quality of bit-spreading. Because many common sets of hashes
   * are already reasonably distributed (so don't benefit from
   * spreading), and because we use trees to handle large sets of
   * collisions in bins, we just XOR some shifted bits in the
   * cheapest possible way to reduce systematic lossage, as well as
   * to incorporate impact of the highest bits that would otherwise
   * never be used in index calculations because of table bounds.
   */
```

- 在get和put的过程中，计算下标时，先对hashCode进行hash操作，然后再通过hash值进一步计算下标，如下图所示：  
  
  - ![](https://cloud.githubusercontent.com/assets/1736354/6957712/293b52fc-d932-11e4-854d-cb47be67949a.png "hash")

- 在设计hash函数时，因为目前的table长度n为2的幂，而计算下标的时候，是这样实现的\(使用`&`位操作，而非`%`求余\)：    
  
  ```java
  (n - 1) & hash
  ```

- 设计者认为这方法很容易发生碰撞。为什么这么说呢？
  
  - 不妨思考一下，在n - 1为15\(0x1111\)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。
  - 因此，设计者想了一个顾全大局的方法\(综合考虑了速度、作用、质量\)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用`O(logn)`的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算\(table长度比较小时\)，从而引起的碰撞。

- 如果还是产生了频繁的碰撞，会发生什么问题呢？
  
  - 作者注释说，他们使用树来处理频繁的碰撞\(we use trees to handle large sets of collisions in bins\)，在[JEP-180](http://openjdk.java.net/jeps/180)中，描述了这个问题：
  - > Improve the performance of java.util.HashMap under high hash-collision conditions by**using balanced trees rather than linked lists to store map entries**. Implement the same improvement in the LinkedHashMap class.

- 之前已经提过，在获取HashMap的元素时，基本分两步：
  
  - 1.首先根据hashCode\(\)做hash，然后确定bucket的index；
  - 2.如果bucket的节点的key不是我们需要的，则通过keys.equals\(\)在链中找。

- 在Java 8之前的实现中是用链表解决冲突的
  
  - 在产生碰撞的情况下，进行get时，两步的时间复杂度是O\(1\)+O\(n\)。因此，当碰撞很厉害的时候n很大，O\(n\)的速度显然是影响速度的。
  - 因此在Java 8中，利用红黑树替换链表，这样复杂度就变成了O\(1\)+O\(logn\)了，这样在n很大的时候，能够比较理想的解决这个问题。[博客](https://github.com/yangchong211/YCBlogs)

- 为什么要这样实现hash？
  
  - 在Java 1.8的实现中，是通过hashCode\(\)的高16位异或低16位实现的：`(h = k.hashCode()) ^ (h >>> 16)`，主要是从**速度、功效、质量**来考虑的，这么做可以在bucket的n比较小的时候，也能保证考虑到高低bit都参与到hash的计算中，同时不会有太大的开销。

### 09.容量和装载因子

- 在HashMap中有两个很重要的参数，容量(Capacity)和负载因子(Load factor)
  
  - 简单的说，Capacity就是bucket的大小，Loadfactor就是bucket填满程度的最大比例。
  - 如果对迭代性能要求很高的话，不要把`capacity`设置过大，也不要把`load factor`设置过小。
  - 当bucket中的entries的数目大于`capacity*load factor`时就需要调整bucket的大小为当前的2倍。

- 什么是装载因子
  
  - 装载因子用于规定数组在自动扩容之前可以数据占有其容量的最高比例，即当数据量占有数组的容量达到这个比例后，数组将自动扩容。
  
  - 装载因子衡量的是一个散列表的空间的使用程度，装载因子越大表示散列表的装填程度越高，反之愈小。因此如果装载因子越大，则对空间的利用程度更高，相对应的是查找效率的降低。
  
  - 如果装载因子太小，那么数组的数据将过于稀疏，对空间的利用率低，官方默认的装载因子为0.75，是平衡空间利用率和运行效率两者之后的结果。
  
  - 如果在实际情况中，内存空间较多而对时间效率要求很高，可以选择降低装载因子的值；如果内存空间紧张而对时间效率要求不高，则可以选择提高装载因子的值。[博客](https://github.com/yangchong211/YCBlogs)
  
  - 此外，即使装载因子和哈希算法设计得再合理，也不免会出现由于哈希冲突导致链表长度过长的情况，这将严重影响 HashMap 的性能。为了优化性能，从 JDK1.8 开始引入了红黑树，当链表长度超出 TREEIFY_THRESHOLD 规定的值时，链表就会被转换为红黑树，利用红黑树快速增删改查的特点以提高 HashMap 的性能。
    
    ```java
    //序列化ID
    private static final long serialVersionUID = 362498820763181265L;
    
    //哈希桶数组的默认容量
    static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;
    
    //网上很多文章都说这个值是哈希桶数组能够达到的最大容量，其实这样说并不准确
    //从 resize() 方法的扩容机制可以看出来，HashMap 每次扩容都是将数组的现有容量增大一倍
    //如果现有容量已大于或等于 MAXIMUM_CAPACITY ，则不允许再次扩容
    //否则即使此次扩容会导致容量超出 MAXIMUM_CAPACITY ，那也是允许的
    static final int MAXIMUM_CAPACITY = 1 << 30;
    
    //装载因子的默认值
    //装载因子用于规定数组在自动扩容之前可以数据占有其容量的最高比例，即当数据量占有数组的容量达到这个比例后，数组将自动扩容
    //装载因子衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高，反之愈小
    //对于使用链表的散列表来说，查找一个元素的平均时间是O(1+a)，因此如果负载因子越大，则对空间的利用程度更高，相对应的是查找效率的降低
    //如果负载因子太小，那么数组的数据将过于稀疏，对空间的利用率低
    //官方默认的负载因子为0.75，是平衡空间利用率和运行效率两者之后的结果
    static final float DEFAULT_LOAD_FACTOR = 0.75f;
    
    //为了提高效率，当链表的长度超出这个值时，就将链表转换为红黑树
    static final int TREEIFY_THRESHOLD = 8;
    ```

- HashMap的大小超过了负载因子(`load factor`)定义的容量，怎么办？
  
  - 如果超过了负载因子(默认**0.75**)，则会重新resize一个原来长度两倍的HashMap，并且重新调用hash方法。

### 10.HashTable和HashMap

- HashTable和HashMap初始化与增长方式
  - 初始化时：
    - HashTable在不指定容量的情况下的默认容量为11，且不要求底层数组的容量一定要为2的整数次幂；HashMap默认容量为16，且要求容量一定为2的整数次幂。
  - 扩容时：[博客](https://github.com/yangchong211/YCBlogs)
    - Hashtable将容量变为原来的2倍加1；HashMap扩容将容量变为原来的2倍。
- HashTable和HashMap线程安全性
  - HashTable其方法函数都是同步的（采用synchronized修饰），不会出现两个线程同时对数据进行操作的情况，因此保证了线程安全性。也正因为如此，在多线程运行环境下效率表现非常低下。因为当一个线程访问HashTable的同步方法时，其他线程也访问同步方法就会进入阻塞状态。比如当一个线程在添加数据时候，另外一个线程即使执行获取其他数据的操作也必须被阻塞，大大降低了程序的运行效率，在新版本中已被废弃，不推荐使用。
  - HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。如果需要同步（1）可以用Collections的synchronizedMap方法；（2）使用ConcurrentHashMap类，相较于HashTable锁住的是对象整体，ConcurrentHashMap基于lock实现锁分段技术。首先将Map存放的数据分成一段一段的存储方式，然后给每一段数据分配一把锁，当一个线程占用锁访问其中一个段的数据时，其他段的数据也能被其他线程访问。ConcurrentHashMap不仅保证了多线程运行环境下的数据访问安全性，而且性能上有长足的提升。

### 11.hashCode和equal

- get和put的中的equals()和hashCode()的都有什么作用？
  - 通过对key的hashCode()进行hashing，并计算下标( `(n-1) & hash`)，从而获得buckets的位置。
  - 如果产生碰撞，则利用key.equals()方法去链表或树中去查找对应的节点

### 12.Key为何需要不可变

- 为什么HashMap中String、Integer这样的包装类适合作为K？
  - String、Integer等包装类的特性能够保证Hash值的不可更改性和计算准确性，能够有效的减少Hash碰撞的几率
    - 都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况
    - 内部已重写了equals()、hashCode()等方法，遵守了HashMap内部的规范（不清楚可以去上面看看putValue的过程），不容易出现Hash值计算错误的情况
    - 如果对象在创建后它的哈希值发生了变化，则Map对象很可能就定位不到映射的位置。
- 想要让自己的Object作为K应该怎么办呢？[博客](https://github.com/yangchong211/YCBlogs)
  - 重写hashCode()和equals()方法
    - 重写hashCode()是因为需要计算存储数据的存储位置，需要注意不要试图从散列码计算中排除掉一个对象的关键部分来提高性能，这样虽然能更快但可能会导致更多的Hash碰撞；
    - 重写equals()方法，需要遵守自反性、对称性、传递性、一致性以及对于任何非null的引用值x，x.equals(null)必须返回false的这几个特性，目的是为了保证key在哈希表中的唯一性；
- 总结
  - 采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。

### 13.HashMap为啥要扩容

- HashMap是为啥要扩容
  - 当链表数组的容量超过初始容量*加载因子（默认0.75）时，再散列将链表数组扩大2倍，把原链表数组的搬移到新的数组中。为什么需要使用加载因子？为什么需要扩容呢？因为如果填充比很大，说明利用的空间很多，如果一直不进行扩容的话，链表就会越来越长，这样查找的效率很低，扩容之后，将原来链表数组的每一个链表分成奇偶两个子链表分别挂在新链表数组的散列位置，这样就减少了每个链表的长度，增加查找效率。
- 重新调整HashMap大小存在什么问题吗？[博客](https://github.com/yangchong211/YCBlogs)
  - 当多线程的情况下，可能产生条件竞争。当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。

### 14.HashMap的table下标

- 它是用自己的hash方法确定下标
  - HashMap自己实现了自己的hash()方法，通过两次扰动使得它自己的哈希值高低位自行进行异或运算，降低哈希碰撞概率也使得数据分布更平均；
  - 在保证数组长度为2的幂次方的时候，使用hash()运算之后的值与运算（&）（数组长度 - 1）来获取数组下标的方式进行存储，这样一来是比取余操作更加有效率，二来也是因为只有当数组长度为2的幂次方时，h&(length-1)才等价于h%length，三来解决了“哈希值与数组大小范围不匹配”的问题；
- 不直接使用hashCode()处理后的哈希值
  - hashCode()方法返回的是int整数类型，其范围为-(2^31)~~(2^31-1)，约有40亿个映射空间，而HashMap的容量范围是在16（初始化默认值）~~2 ^ 30，HashMap通常情况下是取不到最大值的，并且设备上也难以提供这么多的存储空间，从而导致通过hashCode()计算出的哈希值可能不在数组大小范围内，进而无法匹配存储位置；
- 为什么数组长度要保证为2的幂次方呢？
  - 只有当数组长度为2的幂次方时，h&(length-1)才等价于h%length，即实现了key的定位，2的幂次方也可以减少冲突次数，提高HashMap的查询效率；[博客](https://github.com/yangchong211/YCBlogs)
  - 如果 length 为 2 的次幂 则 length-1 转化为二进制必定是 11111……的形式，在于 h 的二进制与操作效率会非常的快，而且空间不浪费；如果 length 不是 2 的次幂，比如 length 为 15，则 length - 1 为 14，对应的二进制为 1110，在于 h 与操作，最后一位都为 0 ，而 0001，0011，0101，1001，1011，0111，1101 这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！这样就会造成空间的浪费。
